{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Cointegration Analysis for Statistical Arbitrage\n",
    "\n",
    "This notebook demonstrates an optimized approach to identifying stock pairs for statistical arbitrage, with a focus on efficiently processing large-scale financial datasets.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "In the context of financial markets, cointegration refers to a statistical relationship between two or more time series where the series move together over the long term, despite short-term deviations. This is particularly valuable in pairs trading strategies, where a stable, mean-reverting relationship between two stocks can be exploited for profit. However, analyzing cointegration on a massive dataset—such as 1 billion rows of 1-minute data across 6,000 stocks—presents significant computational challenges, especially when the tests span multiple years.\n",
    "\n",
    "## Solution\n",
    "\n",
    "### Data Filtering\n",
    "\n",
    "To address the computational demands, the dataset was first filtered to include only those stocks that meet specific criteria (e.g., minimum price, volume, and sufficient historical data). This filtering step, optimized using multiprocessing, rapidly reduced the dataset size, ensuring that only liquid and relevant stocks proceeded to the next stage of analysis.\n",
    "\n",
    "### Efficient Cointegration Testing\n",
    "\n",
    "The Cointegration Augmented Dickey-Fuller (CADF) test was applied to pairs of filtered stocks to identify those that exhibit a cointegrated relationship, crucial for pairs trading. Instead of testing all possible periods, the solution implemented a random sampling approach, conducting CADF tests on 10,000 randomly selected periods. This method provided a comprehensive yet time-efficient overview of potential cointegrated pairs, with the cointegration score reflecting the consistency of this relationship over time.\n",
    "\n",
    "Additionally, multiprocessing was employed to run the analysis concurrently across multiple pairs, significantly reducing the overall processing time.\n",
    "\n",
    "## Impact\n",
    "\n",
    "The approach effectively manages large-scale datasets, achieving a balance between computational efficiency and rigorous analysis. This solution is particularly suitable for developing high-performance trading algorithms that require real-time processing and decision-making in a production environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stock Pair Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2656e1640bca4dd49ef1b5ddd378be54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking for price out of range or low volume:   0%|          | 0/6116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "['AAPL', 'AMD', 'AMZN', 'BABA', 'BAC', 'F', 'GOOG', 'GOOGL', 'INTC', 'MARA', 'PFE', 'PLTR', 'PYPL', 'TSLA', 'UBER', 'XOM']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm.notebook import tqdm\n",
    "import psycopg2\n",
    "\n",
    "def fetch_data_from_db(CONN_STRING, symbol, start_date, end_date, table):\n",
    "    \"\"\"\n",
    "    Fetches minute data from a specified table and date range.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with psycopg2.connect(CONN_STRING) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                query = f\"\"\"\n",
    "                SELECT symbol, timestamp, open, high, low, close, volume\n",
    "                FROM public.{table}\n",
    "                WHERE symbol = %s \n",
    "                AND timestamp BETWEEN %s AND %s\n",
    "                ORDER BY timestamp;\n",
    "                \"\"\"\n",
    "                cur.execute(query, (symbol, start_date, end_date))\n",
    "                columns = ['symbol', 'timestamp', 'open', 'high', \n",
    "                           'low', 'close', 'volume']\n",
    "                data = pd.DataFrame(cur.fetchall(), columns=columns)\n",
    "\n",
    "        data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "        data.set_index('timestamp', inplace=True)\n",
    "        data = data.drop('symbol', axis=1)\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Check if a symbol has any rows with price out of the range, volume \n",
    "# below the minimum, or insufficient data length\n",
    "def has_price_out_of_range_or_low_volume(\n",
    "    symbol, start_date, end_date, conn_string, table_name, \n",
    "    min_stock_price=10, max_stock_price=1000, min_volume=5000, \n",
    "    min_data_length=1000):\n",
    "    \n",
    "    data = fetch_data_from_db(conn_string, symbol, start_date, \n",
    "                              end_date, table_name)\n",
    "    if data.empty or len(data) < min_data_length:\n",
    "        return True \n",
    "\n",
    "    data['avg_price'] = (data['open'] + data['high'] + data['low'] \n",
    "                         + data['close']) / 4\n",
    "    is_price_out_of_range = (data['avg_price'] < min_stock_price).any() \\\n",
    "                            or (data['avg_price'] > max_stock_price).any()\n",
    "    has_low_volume = (data['volume'] < min_volume).any()\n",
    "\n",
    "    return is_price_out_of_range or has_low_volume\n",
    "\n",
    "# Load symbols from a CSV file\n",
    "symbols = pd.read_csv(\n",
    "    '/home/jj/projects/algo_trading/chapter-strategy-optimisation/'\n",
    "    'data/symbols_1m.csv'\n",
    ")\n",
    "symbols = list(symbols['Symbol'].values)\n",
    "\n",
    "# Define constants\n",
    "CONN_STRING = (\"host='192.168.3.41' dbname='proxima' user='airflow' \"\n",
    "               \"password='airflow' port='5432'\")\n",
    "TABLE = 'data_bars_1min_adj_splitdiv'\n",
    "START_DATE = '2024-01-01'\n",
    "END_DATE = '2024-01-15'\n",
    "MIN_STOCK_PRICE = 10\n",
    "MAX_STOCK_PRICE = 300\n",
    "MIN_VOLUME = 2000  # Minimum volume to ensure liquidity\n",
    "MIN_DATA_LENGTH = 100  # Minimum number of rows of data required\n",
    "\n",
    "# Process the symbols in parallel\n",
    "with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "    price_and_volume_flags = list(tqdm(\n",
    "        executor.map(has_price_out_of_range_or_low_volume, symbols, \n",
    "                     [START_DATE]*len(symbols), [END_DATE]*len(symbols), \n",
    "                     [CONN_STRING]*len(symbols), [TABLE]*len(symbols), \n",
    "                     [MIN_STOCK_PRICE]*len(symbols), \n",
    "                     [MAX_STOCK_PRICE]*len(symbols), \n",
    "                     [MIN_VOLUME]*len(symbols), \n",
    "                     [MIN_DATA_LENGTH]*len(symbols)), \n",
    "        total=len(symbols), \n",
    "        desc=\"Checking for price out of range or low volume\"\n",
    "    ))\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Symbol': symbols, \n",
    "    'Has Price Out of Range or Low Volume': price_and_volume_flags\n",
    "})\n",
    "\n",
    "# Filter symbols based on the price range and volume flag\n",
    "filtered_symbols = list(results_df[\n",
    "    results_df['Has Price Out of Range or Low Volume'] == False]['Symbol']\n",
    ")\n",
    "print(len(filtered_symbols))\n",
    "print(filtered_symbols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cointegration Analysis of Stock Pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3450dadcb6ea43249c628faac080180d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/561 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pair</th>\n",
       "      <th>Year</th>\n",
       "      <th>Cointegration Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[GOOG, GOOGL]</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[T, VZ]</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[MARA, RIOT]</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[F, PLTR]</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[F, SQ]</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>[MARA, XOM]</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.077362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>[TSLA, WBA]</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.076954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>[TSLA, XOM]</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.076214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>[AMD, VZ]</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>[TSLA, UBER]</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.072715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>561 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Pair  Year  Cointegration Ratio\n",
       "0    [GOOG, GOOGL]  2023             0.300600\n",
       "1          [T, VZ]  2023             0.141100\n",
       "2     [MARA, RIOT]  2023             0.132800\n",
       "3        [F, PLTR]  2023             0.126500\n",
       "4          [F, SQ]  2023             0.123500\n",
       "..             ...   ...                  ...\n",
       "556    [MARA, XOM]  2023             0.077362\n",
       "557    [TSLA, WBA]  2023             0.076954\n",
       "558    [TSLA, XOM]  2023             0.076214\n",
       "559      [AMD, VZ]  2023             0.075000\n",
       "560   [TSLA, UBER]  2023             0.072715\n",
       "\n",
       "[561 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from datetime import datetime as dt, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import ast\n",
    "sys.path.append(\n",
    "    '/home/jj/anaconda3/envs/stocks/Dropbox/Code/Notebooks/lib/'\n",
    ")\n",
    "from data_fetcher import fetch_data_from_db\n",
    "\n",
    "def fetch_data(symbol, start_date, end_date, conn_string, table_name):\n",
    "    \"\"\"Fetch data from the database.\"\"\"\n",
    "    return fetch_data_from_db(\n",
    "        conn_string, \n",
    "        symbol, \n",
    "        start_date, \n",
    "        end_date, \n",
    "        table_name\n",
    "    )\n",
    "\n",
    "def cadf_test(y, x):\n",
    "    \"\"\"Perform the cointegration test.\"\"\"\n",
    "    cadf_test = coint(y, x)\n",
    "    cadf_stat = cadf_test[0]\n",
    "    cadf_critical_values = cadf_test[2]\n",
    "    return cadf_stat, cadf_critical_values\n",
    "\n",
    "def analyze_pair(pair, start_date, end_date, lookback_period_rows, \n",
    "                 conn_string, table_name, n_periods):\n",
    "    \"\"\"Analyze a single pair of symbols for cointegration.\"\"\"\n",
    "    try:\n",
    "        symbol1, symbol2 = pair\n",
    "        stock_data1 = fetch_data(symbol1, start_date, end_date, \n",
    "                                 conn_string, table_name)\n",
    "        stock_data2 = fetch_data(symbol2, start_date, end_date, \n",
    "                                 conn_string, table_name)\n",
    "\n",
    "        # Ensure both data sets have the same index\n",
    "        stock_data1.index = pd.to_datetime(stock_data1.index)\n",
    "        stock_data2.index = pd.to_datetime(stock_data2.index)\n",
    "\n",
    "        # Concatenate data for the pair into a single DataFrame\n",
    "        df = pd.concat([stock_data1[\"close\"].rename(symbol1), \n",
    "                        stock_data2[\"close\"].rename(symbol2)], axis=1)\n",
    "\n",
    "        # Handle missing data by forward-filling and then backward-filling\n",
    "        df.ffill(inplace=True)\n",
    "        df.bfill(inplace=True)\n",
    "\n",
    "        # Limit the DataFrame to the specified date range\n",
    "        df = df.loc[start_date:end_date]\n",
    "\n",
    "        report_list = []\n",
    "        annual_scores = {}\n",
    "        max_start_index = len(df) - lookback_period_rows\n",
    "        start_indices = random.sample(range(max_start_index), n_periods)\n",
    "\n",
    "        for start_index in start_indices:\n",
    "            end_index = start_index + lookback_period_rows\n",
    "            window_df = df.iloc[start_index:end_index]\n",
    "            y = window_df[symbol1]\n",
    "            x = window_df[symbol2]\n",
    "\n",
    "            # Check if either series is constant\n",
    "            if y.nunique() == 1 or x.nunique() == 1:\n",
    "                continue  # Skip this sample if any series is constant\n",
    "            \n",
    "            # Perform the CADF test to check if the time series are cointegrated.\n",
    "            cadf_stat, cadf_critical_values = cadf_test(y, x)\n",
    "            is_cointegrated = cadf_stat < cadf_critical_values[1]  # 5% critical value\n",
    "            # Determine if the pair is cointegrated by comparing the CADF statistic \n",
    "            # with the 5% critical value.\n",
    "            \n",
    "            start_date = window_df.index[0]\n",
    "            end_date = window_df.index[-1]\n",
    "\n",
    "            report_list.append({\n",
    "                'Pair': [symbol1, symbol2],\n",
    "                'Start Date': start_date, \n",
    "                'End Date': end_date, \n",
    "                'CADF Statistic': cadf_stat,\n",
    "                'Critical Value (5%)': cadf_critical_values[1],\n",
    "                'Cointegrated': is_cointegrated\n",
    "            })\n",
    "\n",
    "            year = start_date.year\n",
    "            if year not in annual_scores:\n",
    "                annual_scores[year] = {'cointegrated': 0, 'total': 0}\n",
    "            annual_scores[year]['total'] += 1\n",
    "            if is_cointegrated:\n",
    "                annual_scores[year]['cointegrated'] += 1\n",
    "\n",
    "        annual_report_list = []\n",
    "        for year in annual_scores:\n",
    "            annual_score = annual_scores[year]['cointegrated'] / \\\n",
    "                           annual_scores[year]['total']\n",
    "            annual_report_list.append({\n",
    "                'Pair': [symbol1, symbol2],\n",
    "                'Year': year,\n",
    "                'Cointegration Ratio': annual_score\n",
    "            })\n",
    "        \n",
    "        return annual_report_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing pair {pair}: {e}\")\n",
    "        return []\n",
    "\n",
    "def analyze_cointegration(symbols, start_date, end_date, \n",
    "                          lookback_period_rows, conn_string, \n",
    "                          table_name, n_periods, max_workers=32):\n",
    "    \"\"\"Analyze cointegration for all pairs of symbols.\"\"\"\n",
    "    symbol_pairs = [(symbols[i], symbols[j]) for i in range(len(symbols)) \n",
    "                    for j in range(i + 1, len(symbols))]\n",
    "\n",
    "    annual_report_list = []\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(analyze_pair, pair, start_date, end_date, \n",
    "                                   lookback_period_rows, conn_string, \n",
    "                                   table_name, n_periods) \n",
    "                   for pair in symbol_pairs]\n",
    "        for future in tqdm(futures, total=len(symbol_pairs)):\n",
    "            annual_report_list.extend(future.result())\n",
    "\n",
    "    annual_report = pd.DataFrame(annual_report_list)\n",
    "\n",
    "    return annual_report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONN_STRING = (\"host='192.168.3.41' dbname='proxima' user='airflow' \"\n",
    "                   \"password='airflow' port='5432'\")\n",
    "    TABLE1M = 'data_bars_1min_adj_splitdiv'\n",
    "    START_DATE = dt(2023, 1, 1)\n",
    "    END_DATE = dt(2023, 12, 1)\n",
    "    LOOKBACK_PERIOD_ROWS = 25  # lookback period for ADF test in number of data points\n",
    "    N_PERIODS = 10000  # Number of random samples to analyze\n",
    "    MAX_WORKERS = 32\n",
    "\n",
    "    annual_report = analyze_cointegration(filtered_symbols[:], START_DATE, \n",
    "                                          END_DATE, LOOKBACK_PERIOD_ROWS, \n",
    "                                          CONN_STRING, TABLE1M, N_PERIODS, \n",
    "                                          MAX_WORKERS)\n",
    "    annual_report = annual_report.sort_values(by='Cointegration Ratio', \n",
    "                                              ascending=False)\n",
    "    \n",
    "    # Convert the 'Pair' column to string format before saving to CSV\n",
    "    annual_report['Pair'] = annual_report['Pair'].apply(str)\n",
    "    annual_report.to_csv('./cointegrated_pairs_2.csv', index=False)\n",
    "   \n",
    "    # To read the DataFrame back with 'Pair' as list\n",
    "    def read_cointegrated_pairs(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Pair'] = df['Pair'].apply(ast.literal_eval)\n",
    "        return df\n",
    "\n",
    "    annual_report_read = read_cointegrated_pairs('./cointegrated_pairs_2.csv')\n",
    "    display(annual_report_read)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant_trade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
