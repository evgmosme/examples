{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Cointegration Analysis for Statistical Arbitrage\n",
    "\n",
    "This notebook demonstrates an optimized approach to identifying stock pairs for statistical arbitrage, with a focus on efficiently processing large-scale financial datasets.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "In the context of financial markets, cointegration refers to a statistical relationship between two or more time series where the series move together over the long term, despite short-term deviations. This is particularly valuable in pairs trading strategies, where a stable, mean-reverting relationship between two stocks can be exploited for profit. However, analyzing cointegration on a massive dataset—such as 1 billion rows of 1-minute data across 6,000 stocks—presents significant computational challenges, especially when the tests span multiple years.\n",
    "\n",
    "## Solution\n",
    "\n",
    "### Data Filtering\n",
    "\n",
    "To address the computational demands, the dataset was first filtered to include only those stocks that meet specific criteria (e.g., minimum price, volume, and sufficient historical data). This filtering step, optimized using multiprocessing, rapidly reduced the dataset size, ensuring that only liquid and relevant stocks proceeded to the next stage of analysis.\n",
    "\n",
    "### Efficient Cointegration Testing\n",
    "\n",
    "The Cointegration Augmented Dickey-Fuller (CADF) test was applied to pairs of filtered stocks to identify those that exhibit a cointegrated relationship, crucial for pairs trading. Instead of testing all possible periods, the solution implemented a random sampling approach, conducting CADF tests on 10,000 randomly selected periods. This method provided a comprehensive yet time-efficient overview of potential cointegrated pairs.\n",
    "\n",
    "Additionally, multiprocessing was employed to run the analysis concurrently across multiple pairs, significantly reducing the overall processing time.\n",
    "\n",
    "## Impact\n",
    "\n",
    "The approach effectively manages large-scale datasets, achieving a balance between computational efficiency and rigorous analysis. This solution is particularly suitable for developing high-performance trading algorithms that require real-time processing and decision-making in a production environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stock Pair Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961963c762514cac8c941a35cae70ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking for price out of range or low volume:   0%|          | 0/6116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'AMD', 'AMZN', 'BABA', 'BAC', 'C', 'CCL', 'COIN', 'CSCO', 'CVX', 'F', 'GOOG', 'GOOGL', 'INTC', 'JPM', 'MARA', 'MU', 'NEE', 'PFE', 'PLTR', 'PYPL', 'RIOT', 'RIVN', 'SHOP', 'SNAP', 'SQ', 'T', 'TSLA', 'UBER', 'VZ', 'WBA', 'WFC', 'WMT', 'XOM']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm.notebook import tqdm\n",
    "import psycopg2\n",
    "import sys\n",
    "sys.path.append('/home/jj/anaconda3/envs/stocks/Dropbox/Code/Notebooks/lib/')\n",
    "from data_fetcher import fetch_data_from_db\n",
    "\n",
    "\n",
    "def fetch_data(symbol, start_date, end_date, conn_string, table_name):\n",
    "    \"\"\"Function to fetch data from the database.\"\"\"\n",
    "    return fetch_data_from_db(\n",
    "        conn_string, \n",
    "        symbol, \n",
    "        start_date, \n",
    "        end_date, \n",
    "        table_name\n",
    "    )\n",
    "\n",
    "def has_price_out_of_range_or_low_volume(\n",
    "    symbol, start_date, end_date, conn_string, table_name, min_stock_price=10, \n",
    "    max_stock_price=1000, min_volume=5000, min_data_length=1000\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Function to check if a symbol has any rows with price out of the range, \n",
    "    volume below the minimum, or insufficient data length\n",
    "    \"\"\"\n",
    "    data = fetch_data(symbol, start_date, end_date, conn_string, table_name)\n",
    "\n",
    "    # If the data is empty or there isn't enough data, the symbol is flagged\n",
    "    if data.empty or len(data) < min_data_length:\n",
    "        return True \n",
    "\n",
    "    # Calculate the average price to determine if the price falls within the acceptable range\n",
    "    data['avg_price'] = (data['open'] + data['high'] + data['low'] + data['close']) / 4\n",
    "    is_price_out_of_range = (data['avg_price'] < min_stock_price).any() or (data['avg_price'] > max_stock_price).any()\n",
    "    # Check if any trading volume is below the minimum threshold\n",
    "    has_low_volume = (data['volume'] < min_volume).any()\n",
    "\n",
    "    # Return True if the symbol is outside the desired range or has low volume\n",
    "    return is_price_out_of_range or has_low_volume\n",
    "\n",
    "symbols = pd.read_csv('/home/jj/projects/algo_trading/chapter-strategy-optimisation/data/symbols_1m.csv')\n",
    "symbols = list(symbols['Symbol'].values)\n",
    "\n",
    "CONN_STRING = \"host='192.168.3.41' dbname='proxima' user='airflow' password='airflow' port='5432'\"\n",
    "TABLE = 'data_bars_1min_adj_splitdiv'\n",
    "START_DATE = '2024-01-01'\n",
    "END_DATE = '2024-01-15'\n",
    "MIN_STOCK_PRICE = 10\n",
    "MAX_STOCK_PRICE = 300\n",
    "MIN_VOLUME = 1000 \n",
    "MIN_DATA_LENGTH = 100\n",
    "\n",
    "# Process the symbols in parallel\n",
    "with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "    price_and_volume_flags = list(tqdm(\n",
    "        # Use ProcessPoolExecutor to speed up the filtering process by checking multiple symbols simultaneously\n",
    "        executor.map(has_price_out_of_range_or_low_volume, symbols, \n",
    "        [START_DATE]*len(symbols), [END_DATE]*len(symbols), \n",
    "        [CONN_STRING]*len(symbols), [TABLE]*len(symbols), \n",
    "        [MIN_STOCK_PRICE]*len(symbols), [MAX_STOCK_PRICE]*len(symbols), \n",
    "        [MIN_VOLUME]*len(symbols), [MIN_DATA_LENGTH]*len(symbols)), \n",
    "        total=len(symbols), desc=\"Checking for price out of range or low volume\"\n",
    "        ))\n",
    "\n",
    "results_df = pd.DataFrame({'Symbol': symbols, 'Has Price Out of Range or Low Volume': price_and_volume_flags})\n",
    "\n",
    "# Filter symbols based on the price range and volume flag\n",
    "filtered_symbols = list(results_df[results_df['Has Price Out of Range or Low Volume'] == False]['Symbol'])\n",
    "print(filtered_symbols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cointegration Analysis of Stock Pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3450dadcb6ea43249c628faac080180d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/561 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from datetime import datetime as dt, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import ast\n",
    "sys.path.append('/home/jj/anaconda3/envs/stocks/Dropbox/Code/Notebooks/lib/')\n",
    "from data_fetcher import fetch_data_from_db\n",
    "\n",
    "def cadf_test(y, x):\n",
    "    \"\"\"Perform the cointegration test.\"\"\"\n",
    "    cadf_test = coint(y, x)\n",
    "    cadf_stat = cadf_test[0]\n",
    "    cadf_critical_values = cadf_test[2]\n",
    "    return cadf_stat, cadf_critical_values\n",
    "\n",
    "def analyze_pair(pair, start_date, end_date, lookback_period_rows, conn_string, table_name, n_periods):\n",
    "    \"\"\"Analyze a single pair of symbols for cointegration.\"\"\"\n",
    "    try:\n",
    "        symbol1, symbol2 = pair\n",
    "        stock_data1 = fetch_data(symbol1, start_date, end_date, conn_string, table_name)\n",
    "        stock_data2 = fetch_data(symbol2, start_date, end_date, conn_string, table_name)\n",
    "\n",
    "        # Ensure both data sets have the same index\n",
    "        # This ensures accurate alignment of data points across both symbols for analysis\n",
    "        stock_data1.index = pd.to_datetime(stock_data1.index)\n",
    "        stock_data2.index = pd.to_datetime(stock_data2.index)\n",
    "\n",
    "        df = pd.concat([stock_data1[\"close\"].rename(symbol1), stock_data2[\"close\"].rename(symbol2)], axis=1)\n",
    "\n",
    "        # Handle missing data by forward-filling and then backward-filling\n",
    "        # This ensures continuity in the data, which is crucial for accurate statistical analysis\n",
    "        df.ffill(inplace=True)\n",
    "        df.bfill(inplace=True)\n",
    "\n",
    "        # Limit the DataFrame to the specified date range\n",
    "        df = df.loc[start_date:end_date]\n",
    "\n",
    "        report_list = []\n",
    "        annual_scores = {}\n",
    "        max_start_index = len(df) - lookback_period_rows\n",
    "        start_indices = random.sample(range(max_start_index), n_periods)\n",
    "\n",
    "        for start_index in start_indices:\n",
    "            end_index = start_index + lookback_period_rows\n",
    "            window_df = df.iloc[start_index:end_index]\n",
    "            y = window_df[symbol1]\n",
    "            x = window_df[symbol2]\n",
    "\n",
    "            # Check if either series is constant\n",
    "            # Avoids errors in statistical tests where variability is required\n",
    "            if y.nunique() == 1 or x.nunique() == 1:\n",
    "                continue  # Skip this sample if any series is constant\n",
    "\n",
    "            cadf_stat, cadf_critical_values = cadf_test(y, x)\n",
    "            is_cointegrated = cadf_stat < cadf_critical_values[1]  # 5% critical value\n",
    "            start_date = window_df.index[0]\n",
    "            end_date = window_df.index[-1]\n",
    "\n",
    "            report_list.append({\n",
    "                'Pair': [symbol1, symbol2],\n",
    "                'Start Date': start_date, \n",
    "                'End Date': end_date, \n",
    "                'CADF Statistic': cadf_stat,\n",
    "                'Critical Value (5%)': cadf_critical_values[1],\n",
    "                'Cointegrated': is_cointegrated\n",
    "            })\n",
    "\n",
    "            # Keep track of the cointegration results by year\n",
    "            year = start_date.year\n",
    "            if year not in annual_scores:\n",
    "                annual_scores[year] = {'cointegrated': 0, 'total': 0}\n",
    "            annual_scores[year]['total'] += 1\n",
    "            if is_cointegrated:\n",
    "                annual_scores[year]['cointegrated'] += 1\n",
    "\n",
    "        annual_report_list = []\n",
    "        for year in annual_scores:\n",
    "            annual_score = annual_scores[year]['cointegrated'] / annual_scores[year]['total']\n",
    "            annual_report_list.append({\n",
    "                'Pair': [symbol1, symbol2],\n",
    "                'Year': year,\n",
    "                'Cointegration Ratio': annual_score\n",
    "            })\n",
    "        \n",
    "        return annual_report_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing pair {pair}: {e}\")\n",
    "        return []\n",
    "\n",
    "def analyze_cointegration(symbols, start_date, end_date, lookback_period_rows, conn_string, table_name, n_periods, max_workers=32):\n",
    "    \"\"\"Analyze cointegration for all pairs of symbols.\"\"\"\n",
    "    symbol_pairs = [(symbols[i], symbols[j]) for i in range(len(symbols)) for j in range(i + 1, len(symbols))]\n",
    "\n",
    "    annual_report_list = []\n",
    "\n",
    "    # Process the symbol pairs in parallel\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(analyze_pair, pair, start_date, end_date, lookback_period_rows, conn_string, table_name, n_periods) for pair in symbol_pairs]\n",
    "        for future in tqdm(futures, total=len(symbol_pairs)):\n",
    "            annual_report_list.extend(future.result())\n",
    "\n",
    "    annual_report = pd.DataFrame(annual_report_list)\n",
    "\n",
    "    return annual_report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONN_STRING = \"host='192.168.3.41' dbname='proxima' user='airflow' password='airflow' port='5432'\"\n",
    "    TABLE1M = 'data_bars_1min_adj_splitdiv'\n",
    "    START_DATE = dt(2023, 1, 1)\n",
    "    END_DATE = dt(2023, 12, 1)\n",
    "    LOOKBACK_PERIOD_ROWS = 25  # lookback period for ADF test in number of data points\n",
    "    N_PERIODS = 10000  # Number of random samples to analyze\n",
    "    MAX_WORKERS = 32\n",
    "\n",
    "    # Perform cointegration analysis on the filtered symbols\n",
    "    annual_report = analyze_cointegration(filtered_symbols[:], START_DATE, END_DATE, LOOKBACK_PERIOD_ROWS, CONN_STRING, TABLE1M, N_PERIODS, MAX_WORKERS)\n",
    "    annual_report = annual_report.sort_values(by='Cointegration Ratio', ascending=False)\n",
    "    \n",
    "    # Convert the 'Pair' column to string format before saving to CSV\n",
    "    annual_report['Pair'] = annual_report['Pair'].apply(str)\n",
    "    annual_report.to_csv('./cointegrated_pairs_2.csv', index=False)\n",
    "   \n",
    "    # To read the DataFrame back with 'Pair' as list\n",
    "    def read_cointegrated_pairs(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Pair'] = df['Pair'].apply(ast.literal_eval)\n",
    "        return df\n",
    "\n",
    "    # Display the final report of cointegrated pairs\n",
    "    annual_report_read = read_cointegrated_pairs('./cointegrated_pairs_2.csv')\n",
    "    display(annual_report_read)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant_trade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
