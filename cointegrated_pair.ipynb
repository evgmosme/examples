{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Pair Filtering and Cointegration Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview: Stock Pair Filtering and Cointegration Analysis\n",
    "\n",
    "This notebook focuses on identifying stock pairs for statistical arbitrage strategies through a two-step process: filtering stocks based on specific criteria and analyzing cointegration between pairs.\n",
    "\n",
    "### 1. Stock Pair Filtering\n",
    "The first script filters over 6,000 stock symbols to ensure they meet certain criteria, such as minimum price, volume, and sufficient historical data. This step ensures that only liquid and relevant stocks are selected for further analysis.\n",
    "\n",
    "### 2. Cointegration Analysis\n",
    "The second script performs cointegration tests on the filtered stock pairs. Cointegration refers to a statistical relationship where two or more time series move together in the long term, despite short-term deviations. This property is essential for pairs trading strategies, as it suggests that the spread between the two stocks may revert to a mean over time.\n",
    "\n",
    "Given the one-minute granularity of the data, performing a CADF (Cointegrated Augmented Dickey-Fuller) test over hundreds of pairs is computationally intensive. To make the process more efficient, the notebook implements a parameter `N_PERIODS = 10,000`, which randomly selects samples for the CADF test. This approach provides an overall picture of the cointegration relationships while significantly reducing the computational time required.\n",
    "\n",
    "### Goal of the Notebook\n",
    "The goal is to identify stock pairs that exhibit a stable, mean-reverting relationship, suitable for pairs trading. The notebook helps build a list of tradeable pairs by filtering stocks and rigorously testing them for cointegration, using an efficient sampling approach to balance accuracy and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stock Pair Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c66e56e1a84e4c832fa77673879e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking for price out of range or low volume:   0%|          | 0/6116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'AMD', 'AMZN', 'BABA', 'BAC', 'F', 'GOOG', 'GOOGL', 'INTC', 'MARA', 'PFE', 'PLTR', 'PYPL', 'TSLA', 'UBER', 'XOM']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm.notebook import tqdm\n",
    "import psycopg2\n",
    "import sys\n",
    "sys.path.append('/home/jj/anaconda3/envs/stocks/Dropbox/Code/Notebooks/lib/')\n",
    "from data_fetcher import fetch_data_from_db\n",
    "\n",
    "# Function to fetch data from the database\n",
    "def fetch_data(symbol, start_date, end_date, conn_string, table_name):\n",
    "    return fetch_data_from_db(\n",
    "        conn_string, \n",
    "        symbol, \n",
    "        start_date, \n",
    "        end_date, \n",
    "        table_name\n",
    "    )\n",
    "\n",
    "# Function to check if a symbol has any rows with price out of the range, volume below the minimum, or insufficient data length\n",
    "def has_price_out_of_range_or_low_volume(\n",
    "    symbol, start_date, end_date, conn_string, table_name, min_stock_price=10, \n",
    "    max_stock_price=1000, min_volume=5000, min_data_length=1000\n",
    "    ):\n",
    "    data = fetch_data(symbol, start_date, end_date, conn_string, table_name)\n",
    "    if data.empty or len(data) < min_data_length:\n",
    "        return True \n",
    "\n",
    "    data['avg_price'] = (data['open'] + data['high'] + data['low'] + data['close']) / 4\n",
    "    is_price_out_of_range = (data['avg_price'] < min_stock_price).any() or (data['avg_price'] > max_stock_price).any()\n",
    "    has_low_volume = (data['volume'] < min_volume).any()\n",
    "\n",
    "    return is_price_out_of_range or has_low_volume\n",
    "\n",
    "# Load symbols from a CSV file\n",
    "symbols = pd.read_csv('/home/jj/projects/algo_trading/chapter-strategy-optimisation/data/symbols_1m.csv')\n",
    "symbols = list(symbols['Symbol'].values)\n",
    "\n",
    "# Define constants\n",
    "CONN_STRING = \"host='192.168.3.41' dbname='proxima' user='airflow' password='airflow' port='5432'\"\n",
    "TABLE = 'data_bars_1min_adj_splitdiv'\n",
    "START_DATE = '2024-01-01'\n",
    "END_DATE = '2024-01-15'\n",
    "MIN_STOCK_PRICE = 10\n",
    "MAX_STOCK_PRICE = 300\n",
    "MIN_VOLUME = 2000  # Minimum volume to ensure liquidity\n",
    "MIN_DATA_LENGTH = 100  # Minimum number of rows of data required\n",
    "\n",
    "# Process the symbols in parallel\n",
    "with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "    price_and_volume_flags = list(tqdm(\n",
    "        executor.map(has_price_out_of_range_or_low_volume, symbols, \n",
    "        [START_DATE]*len(symbols), [END_DATE]*len(symbols), \n",
    "        [CONN_STRING]*len(symbols), [TABLE]*len(symbols), \n",
    "        [MIN_STOCK_PRICE]*len(symbols), [MAX_STOCK_PRICE]*len(symbols), \n",
    "        [MIN_VOLUME]*len(symbols), [MIN_DATA_LENGTH]*len(symbols)), \n",
    "        total=len(symbols), desc=\"Checking for price out of range or low volume\"\n",
    "        ))\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame({'Symbol': symbols, 'Has Price Out of Range or Low Volume': price_and_volume_flags})\n",
    "\n",
    "# Filter symbols based on the price range and volume flag\n",
    "filtered_symbols = list(results_df[results_df['Has Price Out of Range or Low Volume'] == False]['Symbol'])\n",
    "print(filtered_symbols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cointegration Analysis of Stock Pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from datetime import datetime as dt, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import ast\n",
    "sys.path.append('/home/jj/anaconda3/envs/stocks/Dropbox/Code/Notebooks/lib/')\n",
    "from data_fetcher import fetch_data_from_db\n",
    "\n",
    "def fetch_data(symbol, start_date, end_date, conn_string, table_name):\n",
    "    \"\"\"Fetch data from the database.\"\"\"\n",
    "    return fetch_data_from_db(\n",
    "        conn_string, \n",
    "        symbol, \n",
    "        start_date, \n",
    "        end_date, \n",
    "        table_name\n",
    "    )\n",
    "\n",
    "def cadf_test(y, x):\n",
    "    \"\"\"Perform the cointegration test.\"\"\"\n",
    "    cadf_test = coint(y, x)\n",
    "    cadf_stat = cadf_test[0]\n",
    "    cadf_critical_values = cadf_test[2]\n",
    "    return cadf_stat, cadf_critical_values\n",
    "\n",
    "def analyze_pair(pair, start_date, end_date, lookback_period_rows, conn_string, table_name, n_periods):\n",
    "    \"\"\"Analyze a single pair of symbols for cointegration.\"\"\"\n",
    "    try:\n",
    "        symbol1, symbol2 = pair\n",
    "        stock_data1 = fetch_data(symbol1, start_date, end_date, conn_string, table_name)\n",
    "        stock_data2 = fetch_data(symbol2, start_date, end_date, conn_string, table_name)\n",
    "\n",
    "        # Ensure both data sets have the same index\n",
    "        stock_data1.index = pd.to_datetime(stock_data1.index)\n",
    "        stock_data2.index = pd.to_datetime(stock_data2.index)\n",
    "\n",
    "        # Concatenate data for the pair into a single DataFrame\n",
    "        df = pd.concat([stock_data1[\"close\"].rename(symbol1), stock_data2[\"close\"].rename(symbol2)], axis=1)\n",
    "\n",
    "        # Handle missing data by forward-filling and then backward-filling\n",
    "        df.ffill(inplace=True)\n",
    "        df.bfill(inplace=True)\n",
    "\n",
    "        # Limit the DataFrame to the specified date range\n",
    "        df = df.loc[start_date:end_date]\n",
    "\n",
    "        report_list = []\n",
    "        annual_scores = {}\n",
    "        max_start_index = len(df) - lookback_period_rows\n",
    "        start_indices = random.sample(range(max_start_index), n_periods)\n",
    "\n",
    "        for start_index in start_indices:\n",
    "            end_index = start_index + lookback_period_rows\n",
    "            window_df = df.iloc[start_index:end_index]\n",
    "            y = window_df[symbol1]\n",
    "            x = window_df[symbol2]\n",
    "\n",
    "            # Check if either series is constant\n",
    "            if y.nunique() == 1 or x.nunique() == 1:\n",
    "                continue  # Skip this sample if any series is constant\n",
    "\n",
    "            cadf_stat, cadf_critical_values = cadf_test(y, x)\n",
    "            is_cointegrated = cadf_stat < cadf_critical_values[1]  # 5% critical value\n",
    "            start_date = window_df.index[0]\n",
    "            end_date = window_df.index[-1]\n",
    "\n",
    "            report_list.append({\n",
    "                'Pair': [symbol1, symbol2],\n",
    "                'Start Date': start_date, \n",
    "                'End Date': end_date, \n",
    "                'CADF Statistic': cadf_stat,\n",
    "                'Critical Value (5%)': cadf_critical_values[1],\n",
    "                'Cointegrated': is_cointegrated\n",
    "            })\n",
    "\n",
    "            year = start_date.year\n",
    "            if year not in annual_scores:\n",
    "                annual_scores[year] = {'cointegrated': 0, 'total': 0}\n",
    "            annual_scores[year]['total'] += 1\n",
    "            if is_cointegrated:\n",
    "                annual_scores[year]['cointegrated'] += 1\n",
    "\n",
    "        annual_report_list = []\n",
    "        for year in annual_scores:\n",
    "            annual_score = annual_scores[year]['cointegrated'] / annual_scores[year]['total']\n",
    "            annual_report_list.append({\n",
    "                'Pair': [symbol1, symbol2],\n",
    "                'Year': year,\n",
    "                'Cointegration Ratio': annual_score\n",
    "            })\n",
    "        \n",
    "        return annual_report_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing pair {pair}: {e}\")\n",
    "        return []\n",
    "\n",
    "def analyze_cointegration(symbols, start_date, end_date, lookback_period_rows, conn_string, table_name, n_periods, max_workers=32):\n",
    "    \"\"\"Analyze cointegration for all pairs of symbols.\"\"\"\n",
    "    symbol_pairs = [(symbols[i], symbols[j]) for i in range(len(symbols)) for j in range(i + 1, len(symbols))]\n",
    "\n",
    "    annual_report_list = []\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(analyze_pair, pair, start_date, end_date, lookback_period_rows, conn_string, table_name, n_periods) for pair in symbol_pairs]\n",
    "        for future in tqdm(futures, total=len(symbol_pairs)):\n",
    "            annual_report_list.extend(future.result())\n",
    "\n",
    "    annual_report = pd.DataFrame(annual_report_list)\n",
    "\n",
    "    return annual_report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONN_STRING = \"host='192.168.3.41' dbname='proxima' user='airflow' password='airflow' port='5432'\"\n",
    "    TABLE1M = 'data_bars_1min_adj_splitdiv'\n",
    "    START_DATE = dt(2023, 1, 1)\n",
    "    END_DATE = dt(2023, 12, 1)\n",
    "    LOOKBACK_PERIOD_ROWS = 25  # lookback period for ADF test in number of data points\n",
    "    N_PERIODS = 10000  # Number of random samples to analyze\n",
    "    MAX_WORKERS = 32\n",
    "\n",
    "    annual_report = analyze_cointegration(filtered_symbols[:], START_DATE, END_DATE, LOOKBACK_PERIOD_ROWS, CONN_STRING, TABLE1M, N_PERIODS, MAX_WORKERS)\n",
    "    annual_report = annual_report.sort_values(by='Cointegration Ratio', ascending=False)\n",
    "    \n",
    "    # Convert the 'Pair' column to string format before saving to CSV\n",
    "    annual_report['Pair'] = annual_report['Pair'].apply(str)\n",
    "    annual_report.to_csv('./cointegrated_pairs_2.csv', index=False)\n",
    "   \n",
    "    # To read the DataFrame back with 'Pair' as list\n",
    "    def read_cointegrated_pairs(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Pair'] = df['Pair'].apply(ast.literal_eval)\n",
    "        return df\n",
    "\n",
    "    annual_report_read = read_cointegrated_pairs('./cointegrated_pairs_2.csv')\n",
    "    display(annual_report_read)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant_trade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
